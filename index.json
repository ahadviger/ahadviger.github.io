[{"authors":["admin"],"categories":null,"content":"I am a PhD student in computer vision, focused on event-based visionfor mobile robotics. I am a member of the Laboratory for Autonomous Systems and Mobile Robotics (LAMOR) and a teaching assistant at the University of Zagreb.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ahadviger.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student in computer vision, focused on event-based visionfor mobile robotics. I am a member of the Laboratory for Autonomous Systems and Mobile Robotics (LAMOR) and a teaching assistant at the University of Zagreb.","tags":null,"title":"Antea Hadviger","type":"authors"},{"authors":["Antea Hadviger","Igor Cvišić","Ivan Marković","Sacha Vražić","Ivan Petrović"],"categories":[],"content":"","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630540800,"objectID":"323889f44bbd313c0ad8836e5bb450d2","permalink":"https://ahadviger.github.io/publication/event-odometry/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/publication/event-odometry/","section":"publication","summary":"Event-based cameras are biologically inspired sensors that output events, i.e., asynchronous pixel-wise brightness changes in the scene. Their high dynamic range and temporal resolution of a microsecond makes them more reliable than standard cameras in environments of challenging illumination and in high-speed scenarios, thus developing odometry algorithms based solely on event cameras offers exciting new possibilities for autonomous systems and robots. In this paper, we propose a novel stereo visual odometry method for event cameras based on feature detection and matching with careful feature management, while pose estimation is done by reprojection error minimization. We evaluate the performance of the proposed method on two publicly available datasets: MVSEC sequences captured by an indoor flying drone and DSEC outdoor driving sequences. MVSEC offers accurate ground truth from motion capture, while for DSEC, which does not offer ground truth, in order to obtain a reference trajectory on the standard camera frames we used our SOFT visual odometry, one of the highest ranking algorithms on the KITTI scoreboards. We compared our method to the ESVO method, which is the first and still the only stereo event odometry method, showing on par performance on the MVSEC sequences, while on the DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving scenario with default parameters. Furthermore, two important advantages of our method over ESVO are that it adapts tracking frequency to the asynchronous event rate and does not require initialization.","tags":[],"title":"Feature-based Event Stereo Visual Odometry","type":"publication"},{"authors":["Antea Hadviger","Ivan Marković","Ivan Petrović"],"categories":[],"content":"","date":1601027744,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601027744,"objectID":"530c4eba3c24d7df20254cdd69a18ddb","permalink":"https://ahadviger.github.io/publication/disparity-tracking/","publishdate":"2020-09-25T11:55:44+02:00","relpermalink":"/publication/disparity-tracking/","section":"publication","summary":"Event cameras are biologically inspired sensors that asynchronously detect brightness changes in the scene independently for each pixel. Their output is a stream of events which is reported with a low latency and high temporal resolution of a microsecond, making them superior to standard cameras in highly dynamic scenarios when they are sensitive to motion blur. Event cameras can be used in a wide range of applications, one of them being depth estimation, in both stereo and monocular settings. However, most known event-based depth estimation methods yield sparse depth maps due to the nature of the sparse event stream. We present a novel method that fuses information from both events and standard frames, as well as odometry, to exploit the advantages of both sensors. We propose to estimate dense disparity from standard frames at the point of their availability, predict the disparity using odometry information, and track the disparity asynchronously using optical flow of events between the standard frames. We present the performance of the method through several experiments in various setups, including synthetic data, KITTI dataset enhanced with events, MVSEC dataset, as well as our own stereo event camera recordings.","tags":[],"title":"Stereo dense depth tracking based on optical flow using frames and events","type":"publication"},{"authors":["Antea Hadviger","Ivan Marković","Ivan Petrović"],"categories":[],"content":"","date":1568499125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568499125,"objectID":"6fda1db0d22cd0593ac9e27bba40977b","permalink":"https://ahadviger.github.io/publication/event-lifetime/","publishdate":"2019-09-15T00:12:05+02:00","relpermalink":"/publication/event-lifetime/","section":"publication","summary":"Event-based cameras are biologically inspired sensors that output asynchronous pixel-wise brightness changes in the scene called events. They have a high dynamic range and temporal resolution of a microsecond, opposed to standard cameras that output frames at fixed frame rates and suffer from motion blur. Forming stereo pairs of such cameras can open novel application possibilities, since for each event depth can be readily estimated; however, to fully exploit asynchronous nature of the sensor and avoid fixed time interval event accumulation, stereo event lifetime estimation should be employed. In this paper, we propose a novel method for event lifetime estimation of stereo event-cameras, allowing generation of sharp gradient images of events that serve as input to disparity estimation methods. Since a single brightness change triggers events in both event-camera sensors, we propose a method for single shot event lifetime and disparity estimation, with association via stereo matching. The proposed method is approximately twice as fast and more accurate than if lifetimes were estimated separately for each sensor and then stereo matched. Results are validated on real-world data through multiple stereo event-camera experiments.","tags":[],"title":"Stereo Event Lifetime and Disparity Estimation for Dynamic Vision Sensors","type":"publication"},{"authors":["Goran Popović","Antea Hadviger","Ivan Marković","Ivan Petrović"],"categories":[],"content":"","date":1535155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535155200,"objectID":"8c8642fd265e8720185a4481fbc81b8f","permalink":"https://ahadviger.github.io/publication/stereo-disparity/","publishdate":"2018-08-25T00:00:00Z","relpermalink":"/publication/stereo-disparity/","section":"publication","summary":"Computationally efficient moving object detection and depth estimation from a stereo camera is an extremely useful tool for many computer vision applications, including robotics and autonomous driving. In this paper we show how moving objects can be densely detected by estimating disparity using an algorithm that improves complexity and accuracy of stereo matching by relying on information from previous frames. The main idea behind this approach is that by using the ego-motion estimation and the disparity map of the previous frame, we can set a prior base that enables us to reduce the complexity of the current frame disparity estimation, subsequently also detecting moving objects in the scene. For each pixel we run a Kalman filter that recursively fuses the disparity prediction and reduced space semi-global matching (SGM) measurements. The proposed algorithm has been implemented and optimized using streaming single instruction multiple data instruction set and multi-threading. Furthermore, in order to estimate the process and measurement noise as reliably as possible, we conduct extensive experiments on the KITTI suite using the ground truth obtained by the 3D laser range sensor. Concerning disparity estimation, compared to the OpenCV SGM implementation, the proposed method yields improvement on the KITTI dataset sequences in terms of both speed and accuracy.","tags":[],"title":"Computationally Efficient Dense Moving Object Detection Based on Reduced Space Disparity Estimation","type":"publication"}]